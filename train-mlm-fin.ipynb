{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-ru-cleaned.csv\n",
      "/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-pt-cleaned.csv\n",
      "/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-tr-cleaned.csv\n",
      "/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-fr-cleaned.csv\n",
      "/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-it-cleaned.csv\n",
      "/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-es-cleaned.csv\n",
      "/kaggle/input/epoch2-turkish/__results__.html\n",
      "/kaggle/input/epoch2-turkish/model_val_untrained.h5\n",
      "/kaggle/input/epoch2-turkish/__output__.json\n",
      "/kaggle/input/epoch2-turkish/custom.css\n",
      "/kaggle/input/epoch2-turkish/__notebook__.ipynb\n",
      "/kaggle/input/epoch2-turkish/model.h5\n",
      "/kaggle/input/epoch2-turkish/submission.csv\n",
      "/kaggle/input/epoch2-turkish/submission_all.csv\n",
      "/kaggle/input/sub9481/submission.csv\n",
      "/kaggle/input/jigsaw-train-translated-yandex-api/train_yandex.csv\n",
      "/kaggle/input/mlm947/train.csv\n",
      "/kaggle/input/mlm947/model_val_untrained.h5\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/torchvision-nightly+20200416-cp36-cp36m-linux_x86_64.whl\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/__results__.html\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/__output__.json\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/custom.css\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/__notebook__.ipynb\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/torch_xla-nightly+20200416-cp36-cp36m-linux_x86_64.whl\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/submission.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/torch-nightly+20200416-cp36-cp36m-linux_x86_64.whl\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/pytorch-xla-env-setup.py\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/__results___files/__results___15_1.png\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/__results___files/__results___19_1.png\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/__results___files/__results___17_1.png\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_792108.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_778314.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_788030.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_729579.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_772526.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_765284.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_799342.csv\n",
      "/kaggle/input/tpu-inference-super-fast-xlmroberta/node_submissions/submission_0_797465.csv\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/tf_model.h5\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/config.json\n",
      "/kaggle/input/epoch2-portugese/__results__.html\n",
      "/kaggle/input/epoch2-portugese/model_val_untrained.h5\n",
      "/kaggle/input/epoch2-portugese/__output__.json\n",
      "/kaggle/input/epoch2-portugese/custom.css\n",
      "/kaggle/input/epoch2-portugese/__notebook__.ipynb\n",
      "/kaggle/input/epoch2-portugese/model.h5\n",
      "/kaggle/input/epoch2-portugese/submission.csv\n",
      "/kaggle/input/epoch2-portugese/submission_all.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/howling-with-wolf-on-l-genpresse/__results__.html\n",
      "/kaggle/input/howling-with-wolf-on-l-genpresse/__output__.json\n",
      "/kaggle/input/howling-with-wolf-on-l-genpresse/custom.css\n",
      "/kaggle/input/howling-with-wolf-on-l-genpresse/__notebook__.ipynb\n",
      "/kaggle/input/howling-with-wolf-on-l-genpresse/submission.csv\n",
      "/kaggle/input/epoch2-russian/__results__.html\n",
      "/kaggle/input/epoch2-russian/model_val_untrained.h5\n",
      "/kaggle/input/epoch2-russian/__output__.json\n",
      "/kaggle/input/epoch2-russian/custom.css\n",
      "/kaggle/input/epoch2-russian/__notebook__.ipynb\n",
      "/kaggle/input/epoch2-russian/model.h5\n",
      "/kaggle/input/epoch2-russian/submission.csv\n",
      "/kaggle/input/epoch2-russian/submission_all.csv\n",
      "/kaggle/input/encoded-435/x_test_en_blob.npy\n",
      "/kaggle/input/encoded-435/y_train.npy\n",
      "/kaggle/input/encoded-435/x_test.npy\n",
      "/kaggle/input/encoded-435/__results__.html\n",
      "/kaggle/input/encoded-435/x_test_google.npy\n",
      "/kaggle/input/encoded-435/x_train.npy\n",
      "/kaggle/input/encoded-435/__output__.json\n",
      "/kaggle/input/encoded-435/custom.css\n",
      "/kaggle/input/encoded-435/__notebook__.ipynb\n",
      "/kaggle/input/encoded-435/x_valid.npy\n",
      "/kaggle/input/encoded-435/x_test_yandex.npy\n",
      "/kaggle/input/encoded-435/y_valid.npy\n",
      "/kaggle/input/val-correct/valid-pseudo.csv\n",
      "/kaggle/input/val-correct/__results__.html\n",
      "/kaggle/input/val-correct/__output__.json\n",
      "/kaggle/input/val-correct/custom.css\n",
      "/kaggle/input/val-correct/__notebook__.ipynb\n",
      "/kaggle/input/valid-pseudo/val_pseudo.csv\n",
      "/kaggle/input/valid-pseudo/submission.csv\n",
      "/kaggle/input/valid-pseudo/model_val_pseudo.h5\n",
      "/kaggle/input/jigsaw-data/__results__.html\n",
      "/kaggle/input/jigsaw-data/__output__.json\n",
      "/kaggle/input/jigsaw-data/custom.css\n",
      "/kaggle/input/jigsaw-data/__notebook__.ipynb\n",
      "/kaggle/input/jigsaw-data/test_all.csv\n",
      "/kaggle/input/encode-raficko/y_train.npy\n",
      "/kaggle/input/encode-raficko/y_val.npy\n",
      "/kaggle/input/encode-raficko/__results__.html\n",
      "/kaggle/input/encode-raficko/X_test.npy\n",
      "/kaggle/input/encode-raficko/__output__.json\n",
      "/kaggle/input/encode-raficko/custom.css\n",
      "/kaggle/input/encode-raficko/__notebook__.ipynb\n",
      "/kaggle/input/encode-raficko/X_train.npy\n",
      "/kaggle/input/encode-raficko/X_val.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192 \n",
    "DROPOUT = 0.5 # using aggressive dropout to avoid overfitting\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = 2000\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '/kaggle/input/translated-train-bias-all-langs/All languages/train-bias-toxic-google-api-'\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import *\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(D+'test.csv',usecols=['content']).rename(columns={'content':'comment_text'})\n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "train = pd.concat([train,val_df],ignore_index=True)\n",
    "test_df = pd.read_csv('/kaggle/input/jigsaw-data/test_all.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 10s, sys: 1.03 s, total: 3min 11s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "X_test_google = regular_encode(test_df.google.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test_yandex = regular_encode(test_df.yandex.values, tokenizer, maxlen=MAX_LEN)\n",
    "X_test_en_blob = regular_encode(test_df.en_blob.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = np.concatenate((pd.read_csv('/kaggle/input/sub9481/submission.csv')['toxic'].values,\n",
    "                    pd.read_csv('/kaggle/input/val-correct/valid-pseudo.csv')['pseudo'].values)).reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)\n",
    "\n",
    "test_dist_dataset_google  = create_dist_dataset(X_test_google)\n",
    "test_dist_dataset_yandex  = create_dist_dataset(X_test_yandex)\n",
    "test_dist_dataset_en_blob  = create_dist_dataset(X_test_en_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "custom_head (Dense)          (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 48.6 s, sys: 54.3 s, total: 1min 42s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "        model.load_weights('/kaggle/input/mlm947/model_val_untrained.h5')\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "\n",
    "    return compute_loss, train_accuracy_metric\n",
    "\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=2000, validate_every=200):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "    ### Training lopp ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor) \n",
    "        step+=1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Print train metrics ###  \n",
    "            train_metric = train_accuracy_metric.result().numpy()\n",
    "            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n",
    "            \n",
    "            ### Test loop with exact AUC ###\n",
    "            if val_dist_dataset:\n",
    "                val_metric = roc_auc_score(y_val, predict(val_dist_dataset))\n",
    "                print(\"Step %d,   val AUC: %.5f\" %  (step,val_metric))   \n",
    "                \n",
    "                # save weights if it is the best yet\n",
    "                history.append(val_metric)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = model.get_weights()\n",
    "\n",
    "            ### Reset (train) metrics ###\n",
    "            train_accuracy_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "    \n",
    "    ### Restore best weighths ###\n",
    "    model.set_weights(best_weights)\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.experimental_run_v2(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "    \n",
    "    ### get transformer and head separate vars\n",
    "    # get rid of pooler head with None gradients\n",
    "    transformer_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                       if (('pooler' not in v.name)  and \n",
    "                                           ('custom' not in v.name))]\n",
    "    head_trainable_variables = [ v for v in model.trainable_variables \n",
    "                                if 'custom'  in v.name]\n",
    "\n",
    "    # calculate the 2 gradients ( note persistent, and del)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n",
    "    gradients_head = tape.gradient(loss, head_trainable_variables)\n",
    "    del tape\n",
    "        \n",
    "    ### make the 2 gradients steps\n",
    "    optimizer_transformer.apply_gradients(zip(gradients_transformer, \n",
    "                                              transformer_trainable_variables))\n",
    "    optimizer_head.apply_gradients(zip(gradients_head, \n",
    "                                       head_trainable_variables))\n",
    "\n",
    "    train_accuracy_metric.update_state(labels, predictions)\n",
    "\n",
    "\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    ### stack replicas and batches\n",
    "    predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs  # note datasets used in prediction do not have labels\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "compute_loss, train_accuracy_metric = define_losses_and_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.00000\n",
      "Step 200,   val AUC: 0.96127\n",
      "Step 400, train AUC: 0.00000\n",
      "Step 400,   val AUC: 0.96688\n",
      "Step 600, train AUC: 0.00000\n",
      "Step 600,   val AUC: 0.97168\n",
      "Step 800, train AUC: 0.00000\n",
      "Step 800,   val AUC: 0.97344\n",
      "Step 1000, train AUC: 0.00000\n",
      "Step 1000,   val AUC: 0.97646\n",
      "Step 1200, train AUC: 0.00000\n",
      "Step 1200,   val AUC: 0.97941\n",
      "Step 1400, train AUC: 0.00000\n",
      "Step 1400,   val AUC: 0.98068\n",
      "Step 1600, train AUC: 0.00000\n",
      "Step 1600,   val AUC: 0.98218\n",
      "Step 1800, train AUC: 0.00000\n",
      "Step 1800,   val AUC: 0.98340\n",
      "Step 2000, train AUC: 0.00000\n",
      "Step 2000,   val AUC: 0.98481\n",
      "CPU times: user 3min 33s, sys: 1min 6s, total: 4min 39s\n",
      "Wall time: 18min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_val_pseudo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['pseudo'] = predict(val_dist_dataset)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv('val_pseudo.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 17.4 s, total: 1min 26s\n",
      "Wall time: 4min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_fin = sub_df.copy()\n",
    "sub_df['toxic_o'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df['toxic_google'] = predict(test_dist_dataset_google)[:,0]\n",
    "sub_df['toxic_yandex'] = predict(test_dist_dataset_yandex)[:,0]\n",
    "sub_df['toxic_en_blob'] = predict(test_dist_dataset_en_blob)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "pass1 = test_df[test_df.en_blob.str.find(\"pass\") == 0]\n",
    "for i in pass1.id:\n",
    "    if i in sub_df.id:\n",
    "        sub_df.toxic_en_blob.loc[i] = (sub_df.toxic_google.loc[i] + sub_df.toxic_yandex.loc[i] + sub_df.toxic_o.loc[i]) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fin['toxic'] = (0.6*sub_df['toxic_o']) + (0.1*sub_df['toxic_google']) + (0.15*sub_df['toxic_yandex']) + (0.15*sub_df['toxic_en_blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tarek = pd.read_csv('/kaggle/input/howling-with-wolf-on-l-genpresse/submission.csv')\n",
    "sub_shonenkov = pd.read_csv('/kaggle/input/tpu-inference-super-fast-xlmroberta/submission.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['toxic'] = (0.4*sub_tarek['toxic']) + (0.2*sub_shonenkov['toxic']) + (0.4*sub_fin['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
