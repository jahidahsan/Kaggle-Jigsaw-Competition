{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/conda/lib/python3.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 cells are fix for running tensorflow with mixed precision for XLM-R model\n",
    "import json\n",
    "json_path = json.__file__\n",
    "dirs = []\n",
    "for dir_name in json_path.split('/'):\n",
    "    dirs.append(dir_name)\n",
    "    if dir_name.startswith('python'):\n",
    "        break\n",
    "python_lib_path = '/'.join(dirs)\n",
    "python_lib_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fix is done for tensorflow 2.1. Very likely not working for other versions.\n",
    "# However, the current environment should contain tf 2.1, so we don't reinstall tensorflow here.\n",
    "\n",
    "# !pip uninstall -y tensorflow\n",
    "# !pip install --upgrade tensorflow==2.1\n",
    "\n",
    "lines = []\n",
    "with open(python_lib_path + '/site-packages/tensorflow_core/python/keras/layers/normalization.py', \"r\", encoding=\"UTF-8\") as fp:\n",
    "    for line in fp:\n",
    "\n",
    "        if line.strip() == \"outputs = outputs * scale\":\n",
    "            \n",
    "            line_modified = \"        outputs = outputs * math_ops.cast(scale, outputs.dtype)\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        elif line.strip() == \"outputs = outputs + offset\":\n",
    "         \n",
    "            line_modified = \"        outputs = outputs + math_ops.cast(offset, outputs.dtype)\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        elif line == \"      scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\" + \"\\n\":\n",
    "           \n",
    "            lines.append(line)\n",
    "            lines.append(\"      scale = math_ops.cast(scale, inputs.dtype)\" + \"\\n\")\n",
    "            lines.append(\"      offset = math_ops.cast(offset, inputs.dtype)\" + \"\\n\")            \n",
    "\n",
    "        else:\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "with open(python_lib_path + '/site-packages/tensorflow_core/python/keras/layers/normalization.py', \"w\", encoding=\"UTF-8\") as fp:\n",
    "    for line in lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fix is done for tensorflow 2.1. Very likely not working for other versions.\n",
    "# However, the current environment should contain tf 2.1, so we don't reinstall tensorflow here.\n",
    "\n",
    "# !pip uninstall -y tensorflow\n",
    "# !pip install --upgrade tensorflow==2.1\n",
    "\n",
    "lines = []\n",
    "with open(python_lib_path + '/site-packages/tensorflow_core/python/keras/layers/pooling.py', \"r\", encoding=\"UTF-8\") as fp:\n",
    "    for line in fp:\n",
    "\n",
    "        if line.strip() == \"inputs *= mask\":\n",
    "            \n",
    "            lines.append(\"      mask = math_ops.cast(mask, inputs.dtype)\" + \"\\n\")\n",
    "            lines.append(line)\n",
    "\n",
    "        else:\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "with open(python_lib_path + '/site-packages/tensorflow_core/python/keras/layers/pooling.py', \"w\", encoding=\"UTF-8\") as fp:\n",
    "    for line in lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers\n",
    "# !pip install --upgrade transformers==2.9.1\n",
    "\n",
    "lines = []\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_bert.py', \"r\", encoding=\"UTF-8\") as fp:\n",
    "    for line in fp:\n",
    "\n",
    "        if line.strip() == \"embeddings = inputs_embeds + position_embeddings + token_type_embeddings\":\n",
    "\n",
    "            line_modified = \"        embeddings = inputs_embeds + tf.cast(position_embeddings, dtype=inputs_embeds.dtype) + tf.cast(token_type_embeddings, dtype=inputs_embeds.dtype)\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        elif line.strip() == \"attention_scores = attention_scores / tf.math.sqrt(dk)\":\n",
    "\n",
    "            line_modified = \"        attention_scores = attention_scores / tf.cast(tf.math.sqrt(dk), dtype=attention_scores.dtype)\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        else:\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_bert.py', \"w\", encoding=\"UTF-8\") as fp:\n",
    "    for line in lines:\n",
    "        fp.write(line)\n",
    "\n",
    "\n",
    "lines = []\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_bert.py', \"r\", encoding=\"UTF-8\") as fp:\n",
    "    for line in fp:\n",
    "\n",
    "        if line.strip() == \"cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\":\n",
    "\n",
    "            line_modified = \"    cdf = tf.cast(0.5, dtype=x.dtype) * (tf.cast(1.0, dtype=x.dtype) + tf.math.erf(x / tf.cast(tf.math.sqrt(2.0), dtype=x.dtype)))\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        else:\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_bert.py', \"w\", encoding=\"UTF-8\") as fp:\n",
    "    for line in lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_distilbert.py', \"r\", encoding=\"UTF-8\") as fp:\n",
    "    for line in fp:\n",
    "\n",
    "        if line.strip() == \"embeddings = inputs_embeds + position_embeddings  # (bs, max_seq_length, dim)\":\n",
    "\n",
    "            line_modified = \"        embeddings = inputs_embeds + tf.cast(position_embeddings, dtype=inputs_embeds.dtype)  # (bs, max_seq_length, dim)\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        else:\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_distilbert.py', \"w\", encoding=\"UTF-8\") as fp:\n",
    "    for line in lines:\n",
    "        fp.write(line)\n",
    "\n",
    "\n",
    "lines = []\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_distilbert.py', \"r\", encoding=\"UTF-8\") as fp:\n",
    "    for line in fp:\n",
    "\n",
    "        if line.strip() == \"cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\":\n",
    "\n",
    "            line_modified = \"    cdf = tf.cast(0.5, dtype=x.dtype) * (tf.cast(1.0, dtype=x.dtype) + tf.math.erf(x / tf.cast(tf.math.sqrt(2.0), dtype=x.dtype)))\" + \"\\n\"\n",
    "            lines.append(line_modified)\n",
    "\n",
    "        else:\n",
    "\n",
    "            lines.append(line)\n",
    "\n",
    "with open(python_lib_path + '/site-packages/transformers/modeling_tf_distilbert.py', \"w\", encoding=\"UTF-8\") as fp:\n",
    "    for line in lines:\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from transformers import *\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Embedding\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "pd.set_option('max_colwidth',500)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "print(strategy.num_replicas_in_sync)\n",
    "BATCH_SIZE = 44 * strategy.num_replicas_in_sync\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n",
    "maxlen = 192\n",
    "MODEL = '/kaggle/input/jplu-tf-xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/encoded-435/__results__.html\n",
      "/kaggle/input/encoded-435/x_train.npy\n",
      "/kaggle/input/encoded-435/y_train.npy\n",
      "/kaggle/input/encoded-435/x_test_google.npy\n",
      "/kaggle/input/encoded-435/x_test_en_blob.npy\n",
      "/kaggle/input/encoded-435/__notebook__.ipynb\n",
      "/kaggle/input/encoded-435/custom.css\n",
      "/kaggle/input/encoded-435/y_valid.npy\n",
      "/kaggle/input/encoded-435/__output__.json\n",
      "/kaggle/input/encoded-435/x_test_yandex.npy\n",
      "/kaggle/input/encoded-435/x_test.npy\n",
      "/kaggle/input/encoded-435/x_valid.npy\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-data/__results__.html\n",
      "/kaggle/input/jigsaw-data/test_all.csv\n",
      "/kaggle/input/jigsaw-data/__notebook__.ipynb\n",
      "/kaggle/input/jigsaw-data/custom.css\n",
      "/kaggle/input/jigsaw-data/__output__.json\n",
      "/kaggle/input/jplu-tf-xlm-roberta-large/sentencepiece.bpe.model\n",
      "/kaggle/input/jplu-tf-xlm-roberta-large/config.json\n",
      "/kaggle/input/jplu-tf-xlm-roberta-large/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/kaggle/input/jigsaw-data/test_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded from encoded-435 file\n",
    "from numpy import load\n",
    "x_train = load('/kaggle/input/encoded-435/x_train.npy').astype(np.float32)\n",
    "x_valid = load('/kaggle/input/encoded-435/x_valid.npy').astype(np.float32)\n",
    "x_test = load('/kaggle/input/encoded-435/x_test.npy').astype(np.float32)\n",
    "\n",
    "x_test_google = load('/kaggle/input/encoded-435/x_test_google.npy').astype(np.float32)\n",
    "x_test_yandex =load('/kaggle/input/encoded-435/x_test_yandex.npy').astype(np.float32)\n",
    "x_test_en_blob = load('/kaggle/input/encoded-435/x_test_en_blob.npy').astype(np.float32)\n",
    "\n",
    "y_valid = load('/kaggle/input/encoded-435/y_valid.npy').astype(np.float32)\n",
    "\n",
    "y_train = load('/kaggle/input/encoded-435/y_train.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "test_dataset_google = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test_google)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "test_dataset_yandex = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test_yandex)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "test_dataset_en_blob = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test_en_blob)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, loss='binary_crossentropy', max_len=192):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    x = tf.keras.layers.Dropout(0.3)(cls_token)\n",
    "    out = Dense(1)(x)\n",
    "    out = layers.Activation('sigmoid', dtype='float32', name='predictions')(out)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1.6e-5), loss=loss, metrics=[\"accuracy\",tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tfxlm_roberta_model (TFXLMRo ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "predictions (Activation)     (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 31.3 s, sys: 28.2 s, total: 59.4 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#prepare model for training on TPU\n",
    "with strategy.scope():\n",
    "    transformer_layer = transformers.TFXLMRobertaModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer,loss='binary_crossentropy', max_len=maxlen)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1237 steps, validate for 23 steps\n",
      "Epoch 1/2\n",
      "1237/1237 [==============================] - 1435s 1s/step - loss: 0.2042 - accuracy: 0.9140 - auc: 0.9699 - val_loss: 0.3208 - val_accuracy: 0.8808 - val_auc: 0.9003\n",
      "Epoch 2/2\n",
      "1237/1237 [==============================] - 1130s 914ms/step - loss: 0.1480 - accuracy: 0.9396 - auc: 0.9838 - val_loss: 0.3460 - val_accuracy: 0.8753 - val_auc: 0.8927\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "N_STEPS = x_train.shape[0] // BATCH_SIZE\n",
    "EPOCHS = 2\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=N_STEPS,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_val_untrained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 22 steps\n",
      "Epoch 1/2\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.2626 - accuracy: 0.8802 - auc: 0.9068\n",
      "Epoch 2/2\n",
      "22/22 [==============================] - 122s 6s/step - loss: 0.1810 - accuracy: 0.9222 - auc: 0.9576\n"
     ]
    }
   ],
   "source": [
    "# train on valid set\n",
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs= EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 120s 661ms/step\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n",
    "sub_fin = sub.copy()\n",
    "sub['toxic_o'] = model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 70s 384ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic_google'] = model.predict(test_dataset_google, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 61s 334ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic_yandex'] = model.predict(test_dataset_yandex, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 62s 342ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic_en_blob'] = model.predict(test_dataset_en_blob, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass1 = test[test.en_blob.str.find(\"pass\") == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pass1.id:\n",
    "    if i in sub.id:\n",
    "        sub.toxic_en_blob.loc[i] = (sub.toxic_google.loc[i] + sub.toxic_yandex.loc[i] + sub.toxic_o.loc[i]) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_fin['toxic'] = (0.6*sub['toxic_o']) + (0.1*sub['toxic_google']) + (0.15*sub['toxic_yandex']) + (0.15*sub['toxic_en_blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission_all.csv', index=False)\n",
    "sub_fin.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
